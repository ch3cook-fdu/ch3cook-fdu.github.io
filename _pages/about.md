---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

I am currently a third-year Master's student in Electronic Engineering(EE) at Fudan University (Sep. 2021 - Jun. 2024, expected). I am supervised by [Prof. Tao Chen](https://eetchen.github.io/) at [Fudan EDL Lab](https://eetchen.github.io/). Before this, I obtained my Bachelor's degree in Data Science, also from Fudan University (Sep. 2017 - Jun. 2021).

My current research interests include: *3D scene understanding*, *vision and language*, and *multi-modal learning*.


# üî• News
- *2023.10*: &nbsp;üéâüéâ Win the Scan2Cap Challenge in the 3rd Language for 3D Scene Workshop at ICCV 2023. 
- *2023.03*: &nbsp;üéâüéâ One paper is accepted by CVPR 2023. 




# üìù Preprints and Publications 


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2019</div><img src='images/vote2cap-detr++ arXiv.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[FastSpeech: Fast, Robust and Controllable Text to Speech](https://papers.nips.cc/paper/8580-fastspeech-fast-robust-and-controllable-text-to-speech.pdf) \\
**Yi Ren**, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, Tie-Yan Liu

[**Project**](https://speechresearch.github.io/fastspeech/) <strong><span class='show_paper_citations' data='4FA6C0AAAAAJ:qjMakFHDy7sC'></span></strong>

- FastSpeech is the first fully parallel end-to-end speech synthesis model.
- **Academic Impact**: This work is included by many famous speech synthesis open-source projects, such as [ESPNet ![](https://img.shields.io/github/stars/espnet/espnet?style=social)](https://github.com/espnet/espnet). Our work are promoted by more than 20 media and forums, such as [Êú∫Âô®‰πãÂøÉ](https://mp.weixin.qq.com/s/UkFadiUBy-Ymn-zhJ95JcQ)„ÄÅ[InfoQ](https://www.infoq.cn/article/tvy7hnin8bjvlm6g0myu).
- **Industry Impact**: FastSpeech has been deployed in [Microsoft Azure TTS service](https://techcommunity.microsoft.com/t5/azure-ai/neural-text-to-speech-extends-support-to-15-more-languages-with/ba-p/1505911) and supports 49 more languages with state-of-the-art AI quality. It was also shown as a text-to-speech system acceleration example in [NVIDIA GTC2020](https://resources.nvidia.com/events/GTC2020s21420).
</div>
</div>


<!-- <div class='paper-box'><div class='paper-box-image'><div><div class="badge">arXiv preprint</div><img src='images/vote2cap-detr++ arXiv.png' alt="sym" width=100%></div></div><div class='paper-box-text' markdown="1">

[**<font size=4>Vote2Cap-DETR++: Decoupling Localization and Describing for End-to-End 3D Dense Captioning</font>**](https://arxiv.org/abs/2309.02999)

**arXiv**

**<u>Sijin Chen</u>**, [Hongyuan Zhu](https://hongyuanzhu.github.io/), Mingsheng Li, [Xin Chen](https://chenxin.tech/), Peng Guo, Yinjie Lei, [Gang Yu](https://www.skicyyu.org/), Taihao Li, [Tao Chen](https://eetchen.github.io/)$^{\dagger}$

[paper](https://arxiv.org/abs/2309.02999) \| [github](https://github.com/ch3cook-fdu/Vote2Cap-DETR)

- Decoupled feature extraction and task decoding for 3D Dense Captioning.
</div>
</div> -->


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2023</div><img src='images/vote2cap-detr cvpr2023.png' alt="sym" width=100%></div></div><div class='paper-box-text' markdown="1">

[**<font size=4>End-to-End 3D Dense Captioning with Vote2Cap-DETR</font>**](https://arxiv.org/abs/2301.02508)

**CVPR 2023**

**<u>Sijin Chen</u>**, [Hongyuan Zhu](https://hongyuanzhu.github.io/), [Xin Chen](https://chenxin.tech/), Yinjie Lei, [Gang Yu](https://www.skicyyu.org/), [Tao Chen](https://eetchen.github.io/)$^{\dagger}$

[paper](https://arxiv.org/abs/2301.02508) \| [github](https://github.com/ch3cook-fdu/Vote2Cap-DETR) \| [youtube](https://www.youtube.com/watch?v=azR_OvPWYfo)

- We address 3D Dense Captioning as a set prediction problem with parallel decoding.
</div>
</div>




# üéñ Awards and Scholarships

- *Oct. 2023*. 1st place of the Scan2Cap Challenge in the 3rd Language for 3D Scene Workshop at ICCV 2023.
- *Sep. 2022 - Jul.2023*. 2nd prize of the Scholarship for Oustanding Students of Master's Degrees at Fudan University.
- *Sep. 2021 - Jul.2022*. Award for the Scholarship for Outstanding Students of Master's Degrees at Fudan University.
- *Sep. 2020 - Jul.2021*. 2nd prize of the Scholarship for Outstanding Students at Fudan University, *2020 - 2021*.




# üìñ Educations
- *2021.09 - 2024.06 (expected)*, Master student at Fudan University. 
- *2017.09 - 2021.06*, Bachelor student at Fudan University. 




# üí¨ Invited Talks
- *2023.06*, Paper presentation for "*End-to-End 3D Dense Captioning with Vote2Cap-DETR*" at the Workshop for Advances in 3D Vision, VALSE 2023, Wuxi, China. 



# üíª Internships
- *2020.07 - 2020.10*, Huawei Technologies Shanghai R&D Center, Shanghai, China.